# -*- coding: utf-8 -*-
"""Team 8: Module Test-(Part 1)-AAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bhUmqIB8gooXj7b9zharP3jEZjjc-p3W

###Business Objectives
This case study aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicants using EDA is the aim of this case study.
 
In other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default.  The company can utilise this knowledge for its portfolio and risk assessment.
To develop your understanding of the domain, you are advised to independently research a little about risk analytics - understanding the types of variables and their significance should be enough)

##Importing all Require libraries for the case study:
"""

import numpy as np
import pandas as pd
import os
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
import statsmodels
import statsmodels.api as sm
from scipy.stats import kurtosis
import scipy

"""# Importing Data Sets"""

# 1. 'application_data.csv'  contains all the information of the client at the time of application.
#The data is about whether a client has payment difficulties.

df=pd.read_csv("application_data (1).csv")

#2. 'previous_application.csv' contains information about the clientâ€™s previous loan data.
# It contains the data whether the previous application had been Approved, Canceled, Refused or Unused offer.

pa_df=pd.read_csv("previous_application.csv")

#3. 'columns_description.csv' is a data dictionary which describes the meaning of the variables.

col_df=pd.read_csv("columns_description.csv", encoding='latin1')

"""###Viewing Data"""

#checking first 5 rows of columns description data set
col_df.head()

#Checking names of rows of all the data sets
col_df['Row']

#chaking shape of columns description data set
col_df.shape

#Displaying the column names, Non-Null Count in each column, and data type of respective column of dataframe using the function df.info
col_df.info()

#Cheking first 5 rows of application_data
df.head()

#Cheking first 5 rows of previous_application data
pa_df.head()

#chcking shape of Application data
df.shape

#checking Dimention of Application data
df.ndim

#Checking last 5 rows of Application data
df.tail()

#checking Data types for Application Data
df.dtypes

#Checking the column names, Non-Null Count in each column, and data type of respective column of dataframe using the function df.info
df.info()

#Gives statestical Description of continues data
df.describe()

#Gives statestical Description of all columns including catorgerical data
df.describe(include='all')

"""#2:: Data Wrangling

###As data is having huge number of lines, we will analyze data in parts
"""

#defining new Data frame using limited number of columns
new_df=df[['SK_ID_CURR','TARGET','DAYS_EMPLOYED','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','FLAG_MOBIL','FLAG_EMAIL','OCCUPATION_TYPE','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','ORGANIZATION_TYPE','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','APARTMENTS_AVG','BASEMENTAREA_AVG','DAYS_LAST_PHONE_CHANGE']]

#checking first 5 rows of new data frame
new_df.head()

#cheking shape of new data, we have now reduced our no of columns from 122 to 30
new_df.shape

#checking names of columns of new data
new_df.columns

"""####Renaming the column names"""

list=['Cust_ID','TARGET','DAYS_EMPLOYED','INCOME', 'Loan_AMT', 'ANNUITY',
       'GOODS_PRICE', 'INCOME_TYPE', 'EDUCATION',
       'FAMILY_STATUS', 'curr_HOUSING_TYPE', 'age',
       'Work_Exp', 'REGISTRATION_change', 'DAYS_ID_PUBLISH', 'MOBIL_given',
       'EMAIL_given', 'OCCUPATION_TYPE', 'Family_MEMBERS_no',
       'REGION_CLIENT', 'REGION_CLIENT_CITY',
       'Perman_add_NOT_cont_REGION', 'perman_add_NOT_WORK_add',
       'ORGANIZATION_TYPE', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',
       'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'LAST_PHONE_CHANGE']

new_df.columns=list

new_df.columns

#checking duplicate rows in a data set
new_df.duplicated().sum()

"""##Checking for null values in columns"""

def null_values(new_df):
    return round((new_df.isnull().sum()*100/len(new_df)).sort_values(ascending = False),2)

#displaying columns with Percentage of null values
null_values(new_df)

"""BASEMENTAREA_AVG              58.52
EXT_SOURCE_1                  56.38

APARTMENTS_AVG                50.75

OCCUPATION_TYPE               31.35

EXT_SOURCE_3                  19.83

these are the columns having maximum null values
"""

#number of null values per column
print("missing values : ",new_df.isna().sum().sort_values(ascending = False))

"""##Replacing null values 
###TO deal with missing data?

Drop data:

        a. Drop the whole row

        b. Drop the whole column
 
Replace data

        a. Replace it by mean
        
        b. Replace it by frequency
        
        c. Replace it based on other functions
"""

#Replacing null values with NAN 
new_df=new_df.replace(np.nan, 'NAN')

"""###Understanding the Categorical variables"""

#taking info to check Column name,  Non-Null Count, Dtype and sape of data (307511X28 )
new_df.info()

"""##Checking statisticsal summary of all the columns

"""

#statistical summary of all the numeric columns
new_df.describe()

#checking correlation between columns of numeric data
new_df.corr()

#statistical summary of all the non numeric columns
new_df.describe(include='all')

"""#Checking unique values for catogorical columns and Visualizing data"""

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

"""###Target variable 
(1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)
"""

#unique value INCOME_TYPE
new_df.TARGET.unique()

#count of unique value INCOME_TYPE   
v=new_df.TARGET.value_counts()
v

plt.barh(v.index, v)

"""### Calculating Imbalance percentage using Target Variable"""

plt.pie(x=v, labels=v.index, autopct='%1.2f%%')

#It shows that 8.15% people do not repay there loan

"""###INCOME_TYPE"""

#unique value INCOME_TYPE
new_df.INCOME_TYPE.unique()

#count of unique value INCOME_TYPE   
a=new_df.INCOME_TYPE.value_counts()
a

plt.barh(a.index, a)

#here we see that most number of people who take Loan fall under Income_type (occupation)==>Working (158774), Commercial associate (71617),
#Pensioner (55362), State servant (21703)

#Showcasing  Income_type typr of people along with there Percentage w.r.t total number of people taking loan
plt.pie(x=a, labels=a.index, autopct='%1.2f%%')


plt.figure(figsize = [20,20])
plt.show()

"""###EDUCATION"""

new_df.EDUCATION.unique()

b=new_df.EDUCATION.value_counts()
b
#maximum people who take loan are having Education asSecondary / secondary special with count of 218391

plt.barh(b.index, b)

plt.pie(x=b, labels=b.index, autopct='%1.2f%%')

plt.show()

"""###FAMILY_STATUS"""

new_df.FAMILY_STATUS.unique()

c=new_df.FAMILY_STATUS.value_counts()
c

plt.barh(c.index, c)

plt.pie(x=c, labels=c.index, autopct='%1.2f%%')

plt.show()

"""###curr_HOUSING_TYPE"""

new_df.curr_HOUSING_TYPE.unique()

d=new_df.curr_HOUSING_TYPE.value_counts()
d

plt.barh(d.index, d)

plt.pie(x=d, labels=d.index, autopct='%1.2f%%')

plt.show()

"""###ORGANIZATION_TYPE"""

new_df.ORGANIZATION_TYPE.unique()

plt.pyplot.hist(new_df["ORGANIZATION_TYPE"])

new_df.ORGANIZATION_TYPE.value_counts()

"""##for catogorical data"""

#Checking the data Types
new_df.dtypes

#taking only continues data into account and making new data frame
cat_df=new_df[['Cust_ID','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

cat_df.head()

#Checking Corrilation Between columns of numric continues data
cat_df.corr()



"""##Ploting pairplot  for continues data to check the retaion between columns"""

sns.pairplot(data=cat_df)

"""#Handling Outliers

####INCOME
"""

#Checking outliers for INCOME
cat_df.INCOME.describe()

sns.boxplot(cat_df.INCOME)
plt.title('Distribution of INCOME')
plt.show()

"""#Loan_AMT"""

#Checking outliers for Loan Amount
cat_df.Loan_AMT.describe()

sns.boxplot(cat_df.Loan_AMT)
plt.title('Distribution of INCOME')
plt.show()

#Filterring data on bases of Laon amount for top 10 loan amounts
df_amt=cat_df.nlargest(10,columns='Loan_AMT')
df_amt.head()

#Plotting graph for approved loan amount 
df_amt.plot.bar()

"""#ANNUITY"""

#Checking outliers for ANNUITY
cat_df.ANNUITY.describe()

cat_df.shape

print("missing values from Annuty : ",cat_df.ANNUITY.isna().sum())



"""#GOODS_PRICE"""

#Checking outliers for GOODS_PRICE
cat_df.GOODS_PRICE.describe()



"""#Find the top 10 correlation for the Client with payment difficulties and all other cases (Target variable).

"""

new_df.TARGET.unique()

r=new_df.TARGET.value_counts()
r

plt.barh(r.index, r)

plt.pie(x=r, labels=r.index, autopct='%1.2f%%')
plt.legend()
plt.show()

"""##Conclustion:: 8.07% of people have payment difficulty"""

cat_df.head()

"""# Binary Logistic Regression(taking into account Target as response variable)

"""

dff=new_df[['Cust_ID','TARGET','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

"""General Regression::
at least 1 catagorical predictor(we create dummy var) and respoce is continues(we have target variable as catogorical data)
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
import sklearn
from sklearn.model_selection import train_test_split

mydf=dff[['TARGET','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

#x_train, x_test, y_train, y_test = train_test_split(mydf.drop('TARGET', axis=1), mydf.TARGET, test_size=0.2)

"""#checking for Target var vs Income"""

x = mydf[['INCOME']]
y=mydf[['TARGET']]

x=sm.add_constant(x)

x

mod = sm.Logit(y,x).fit()

print(mod.summary())

"""#p=value is <0.05 so Ho is rejected and it means that Income affect the person will default or not on loan repayment

#checking for Target var vs Loan_AMT
"""

x = mydf[['Loan_AMT']]
y=mydf[['TARGET']]

x=sm.add_constant(x)

x

mod1 = sm.Logit(y,x).fit()

print(mod1.summary())

"""#p=value is <0.05 so Ho is rejected and it means that Loan_AMT  affect the person will default or not on loan repayment"""