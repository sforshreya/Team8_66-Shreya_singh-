# -*- coding: utf-8 -*-
"""Team 8: Module Test-(Part 1)-AAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bhUmqIB8gooXj7b9zharP3jEZjjc-p3W
"""



"""Import all libraries """

import numpy as np
import pandas as pd
import os
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
import statsmodels
import statsmodels.api as sm
from scipy.stats import kurtosis
import scipy

"""#Importing Data from CSV file"""

df=pd.read_csv("application_data (1).csv")

pa_df=pd.read_csv("previous_application.csv")

col_df=pd.read_csv("columns_description.csv", encoding='latin1')

col_df.head()

col_df['Row']

col_df.shape

"""#viewing data"""

df.head()

pa_df.head()

df.shape

df.ndim

df.tail()

df.dtypes

df.info()

##gives statestical Description of continues data
df.describe()

#gives statestical Description of catorgerical data
df.describe(include='all')

"""#2:: Data Preparation

##as data is having huge number of lines, we are considering appicable columns only by looking at data set
"""

new_df=df[['SK_ID_CURR','TARGET','DAYS_EMPLOYED','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','FLAG_MOBIL','FLAG_EMAIL','OCCUPATION_TYPE','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','ORGANIZATION_TYPE','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','APARTMENTS_AVG','BASEMENTAREA_AVG','DAYS_LAST_PHONE_CHANGE']]

new_df.head()

#cheking shape of new data, we have now reduced our no of columns from 122 to 28
new_df.shape

#checking names of columns of new data
new_df.columns

"""####reanaming the column names"""

list=['Cust_ID','TARGET','DAYS_EMPLOYED','INCOME', 'Loan_AMT', 'ANNUITY',
       'GOODS_PRICE', 'INCOME_TYPE', 'EDUCATION',
       'FAMILY_STATUS', 'curr_HOUSING_TYPE', 'age',
       'Work_Exp', 'REGISTRATION_change', 'DAYS_ID_PUBLISH', 'MOBIL_given',
       'EMAIL_given', 'OCCUPATION_TYPE', 'Family_MEMBERS_no',
       'REGION_CLIENT', 'REGION_CLIENT_CITY',
       'Perman_add_NOT_cont_REGION', 'perman_add_NOT_WORK_add',
       'ORGANIZATION_TYPE', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',
       'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'LAST_PHONE_CHANGE']

new_df.columns=list

new_df.columns

"""##checking for columns with null values"""

def null_values(new_df):
    return round((new_df.isnull().sum()*100/len(new_df)).sort_values(ascending = False),2)

#displaying columns with Percentage of null values
null_values(new_df)

"""BASEMENTAREA_AVG              58.52
EXT_SOURCE_1                  56.38

APARTMENTS_AVG                50.75

OCCUPATION_TYPE               31.35

EXT_SOURCE_3                  19.83

these are the columns having maximum null values
"""

#number of null values per column
print("missing values : ",new_df.isna().sum().sort_values(ascending = False))

"""##Replacing null values with NAN"""

#Replacing null values with NAN for all the columns
new_df=new_df.replace(np.nan, 'NAN')

"""###We reaplaced all empty columns with NAN

#Understanding of the variables Categorical variables
"""

#taking info to check Column name,  Non-Null Count,   Dtype and sape of data (307511X28 )
new_df.info()

"""#cheking statisticsal summary of all the columns

"""

#statisticsal summary of all the numric columns
new_df.describe()

#checking corrilation between columns of numric data
new_df.corr()

#statisticsal summary of all the non numric columns
new_df.describe(include='all')

"""#Checking unique values for catogorical columns and Visualizing data"""

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

"""###Target variable 
(1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)
"""

#unique value INCOME_TYPE
new_df.TARGET.unique()

#count of unique value INCOME_TYPE   
v=new_df.TARGET.value_counts()
v

plt.barh(v.index, v)

#unique value INCOME_TYPE
new_df.INCOME_TYPE.unique()

#count of unique value INCOME_TYPE   
a=new_df.INCOME_TYPE.value_counts()
a

plt.barh(a.index, a)

plt.pie(x=a, labels=a.index, autopct='%1.2f%%')
plt.legend()
plt.show()

new_df.EDUCATION.unique()

b=new_df.EDUCATION.value_counts()
b

plt.barh(b.index, b)

plt.pie(x=b, labels=b.index, autopct='%1.2f%%')
plt.legend()
plt.show()

new_df.FAMILY_STATUS.unique()

c=new_df.FAMILY_STATUS.value_counts()
c

plt.barh(c.index, c)

plt.pie(x=c, labels=c.index, autopct='%1.2f%%')
plt.legend()
plt.show()

new_df.curr_HOUSING_TYPE.unique()

d=new_df.curr_HOUSING_TYPE.value_counts()
d

plt.barh(d.index, d)

plt.pie(x=d, labels=d.index, autopct='%1.2f%%')
plt.legend()
plt.show()

new_df.ORGANIZATION_TYPE.unique()

plt.pyplot.hist(new_df["ORGANIZATION_TYPE"])

new_df.ORGANIZATION_TYPE.value_counts()

"""##for catogorical data"""

new_df.dtypes

#taking only continues data into account
cat_df=new_df[['Cust_ID','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

cat_df.head()

#Checking Corrilation Between columns of numric continues data
cat_df.corr()



"""#ploting pairplot  for continues data to check the retaion between columns"""

sns.pairplot(data=cat_df)

"""#Handling Outliers"""

##INCOME

#Checking outliers for INCOME
cat_df.INCOME.describe()

sns.boxplot(cat_df.INCOME)
plt.title('Distribution of INCOME')
plt.show()

"""#Loan_AMT"""

#Checking outliers for Loan Amount
cat_df.Loan_AMT.describe()

sns.boxplot(cat_df.Loan_AMT)
plt.title('Distribution of INCOME')
plt.show()

#Filterring data on bases of Laon amount for top 10 loan amounts
df_amt=cat_df.nlargest(10,columns='Loan_AMT')
df_amt.head()

#Plotting graph for approved loan amount 
df_amt.plot.bar()

"""#ANNUITY"""

#Checking outliers for ANNUITY
cat_df.ANNUITY.describe()

cat_df.shape

ann=cat_df.ANNUITY.dropna()

cat_df.shape

cat_df.ANNUITY.dropna()

print("missing values from Annuty : ",cat_df.ANNUITY.isna().sum())



"""#GOODS_PRICE"""

#Checking outliers for GOODS_PRICE
cat_df.GOODS_PRICE.describe()



"""#Find the top 10 correlation for the Client with payment difficulties and all other cases (Target variable).

"""

new_df.TARGET.unique()

r=new_df.TARGET.value_counts()
r

plt.barh(r.index, r)

plt.pie(x=r, labels=r.index, autopct='%1.2f%%')
plt.legend()
plt.show()

"""##Conclustion:: 8.07% of people have payment difficulty"""

cat_df.head()

"""# Binary Logistic Regression(taking into account Target as response variable)

"""

dff=new_df[['Cust_ID','TARGET','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

"""General Regression::
at least 1 catagorical predictor(we create dummy var) and respoce is continues(we have target variable as catogorical data)
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
import sklearn
from sklearn.model_selection import train_test_split

mydf=dff[['TARGET','INCOME','Loan_AMT','ANNUITY','GOODS_PRICE']]

#x_train, x_test, y_train, y_test = train_test_split(mydf.drop('TARGET', axis=1), mydf.TARGET, test_size=0.2)

"""#checking for Target var vs Income"""

x = mydf[['INCOME']]
y=mydf[['TARGET']]

x=sm.add_constant(x)

x

mod = sm.Logit(y,x).fit()

print(mod.summary())

"""#p=value is <0.05 so Ho is rejected and it means that Income affect the person will default or not on loan repayment

#checking for Target var vs Loan_AMT
"""

x = mydf[['Loan_AMT']]
y=mydf[['TARGET']]

x=sm.add_constant(x)

x

mod1 = sm.Logit(y,x).fit()

print(mod1.summary())

"""#p=value is <0.05 so Ho is rejected and it means that Loan_AMT  affect the person will default or not on loan repayment"""