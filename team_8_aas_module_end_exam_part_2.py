# -*- coding: utf-8 -*-
"""TEAM 8 -AAS Module End Exam- part 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15QIwuigbdO7U3PRqjjuy_jS4yif265_e
"""

import numpy as np
import pandas as pd
import os
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
import statsmodels
import statsmodels.api as sm
from scipy.stats import kurtosis
import scipy

#for importing datasets uploaded on the google drive
from google.colab import drive
drive.mount('/content/drive/')

#to view the folder import from the google drive 
!ls "/content/drive/MyDrive/AAS module end exam/"

#assigning path variable to path of the data sets
path1=  "/content/drive/MyDrive/AAS module end exam/application_data.csv"
path2=  "/content/drive/MyDrive/AAS module end exam/previous_application.csv"

"""# Importing Data from CSV"""

# importing data set 'application_data' from csv from the drive 
appl_data = pd.read_csv(path1)

# importing "previous_application" data set from csv the drive
pa_df=pd.read_csv(path2)

# importing "columns_description" data set from csv 
col_df=pd.read_csv("columns_description.csv", encoding='latin1')

"""# Viewing the Dataset"""

#checking first five rows and columns of the application_data dataset
appl_data.head()

#checking first five rows and columns of the previous_application dataset
pa_df.head()

#checking first five rows and columns of the columns_description dataset
col_df.head()

#checking the shape of the dataset "application_data"
appl_data.shape

#checking the shape of the dataset "previous_application"
pa_df.shape

#checking the shape of the dataset "columns_description"
col_df.shape

#checking the dimension of the "application_data" data set -- We have a two dimension dataset
appl_data.ndim

#checking the dimension of the "previous_application" data set -- We have a two dimension dataset
pa_df.ndim

col_df['Row']

#checking last five rows and columns of the raw dataset--"application_data"
appl_data.tail()

#checking last five rows and columns of the raw dataset--"previous_data"
pa_df.tail()

#checking the data types of columns of data set "application_data"
appl_data.dtypes

#checking the data types of columns of data set "previous_application"
pa_df.dtypes

'''it provides purely descriptive information about the dataset. 
This information includes statistics that summarize the central tendency of the variable,
 their dispersion, the presence of empty values and their shape'''
appl_data.info()

pa_df.info()

# This shows the statistical summary of all numeric-typed (int, float) columns
# Describe 
appl_data.describe()

pa_df.describe()

# checking all the columns including those that are of type object
appl_data.describe(include='all')

# checking all the columns including those that are of type object
pa_df.describe(include='all')

"""# Data Preparation"""

appl_data.columns

"""As data is having huge number of lines, we are considering appicable columns only by looking at data set"""

#creating a new data set using the columns of appl_data

new_df=appl_data[['SK_ID_CURR', 'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',
           'AMT_GOODS_PRICE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE',
           'NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','DAYS_BIRTH',
           'DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH',
           'FLAG_MOBIL','FLAG_EMAIL','OCCUPATION_TYPE','CNT_FAM_MEMBERS',
           'REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY',
           'REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION',
           'ORGANIZATION_TYPE','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3',
           'APARTMENTS_AVG','BASEMENTAREA_AVG','DAYS_LAST_PHONE_CHANGE']]

newtest_df= pa_df[['SK_ID_PREV','SK_ID_CURR','NAME_CONTRACT_TYPE','AMT_ANNUITY',
                   'AMT_APPLICATION','AMT_CREDIT','AMT_DOWN_PAYMENT','AMT_GOODS_PRICE',
                   'NFLAG_LAST_APPL_IN_DAY','RATE_DOWN_PAYMENT','RATE_INTEREST_PRIMARY',
                   'RATE_INTEREST_PRIVILEGED','NAME_CASH_LOAN_PURPOSE','NAME_CONTRACT_STATUS',
                   'NAME_PAYMENT_TYPE','CODE_REJECT_REASON','NAME_CLIENT_TYPE','NAME_GOODS_CATEGORY',
                   'NAME_PORTFOLIO','CHANNEL_TYPE','NAME_SELLER_INDUSTRY','NAME_YIELD_GROUP',
                   'DAYS_FIRST_DUE','DAYS_LAST_DUE','DAYS_TERMINATION','NFLAG_INSURED_ON_APPROVAL']]

#creating a new data set using the columns of pa_df

list2= ['PREV_CustID', 
'CURR_CustID',
'CONTRACT_TYPE',
'AMT_ANNUITY',
'AMT_APPLICATION',
'AMT_CREDIT',
'AMT_DOWN_PAYMENT',
'AMT_GOODS_PRICE',
'LASTAPPL_PerDAY',
'RATE_DOWN_PAYMENT',
'RATE_INTEREST_PRIMARY',
'RATE_INTEREST_PRIVILEGED',
'CASH_LOAN_PURPOSE',
'CONTRACT_STATUS',
'PAYMENT_TYPE',
'CODE_REJECT_REASON',
'CLIENT_TYPE',
'GOODS_CATEGORY',
'PORTFOLIO',
'CHANNEL_TYPE',
'SELLER_INDUSTRY',
'YIELD_GROUP',
'FIRST_DUEDay',
'LAST_DUEDay',
'DAYS_TERMINATION',
'INSURED_ON_APPROVAL']

#new dataframe created for pa_df -- "previous_application" with change in column names 
newtest_df.columns= list2

#checking if the list2 concatenated to new dataframe has changed the column names
newtest_df.head()

# checking the new dataframe of appl_data -- "application_data"
new_df.head()

#checking shape of new data of appl_data , we have now reduced our no of columns from 122 to 28
new_df.shape

#checking names of columns of new data
new_df.columns

"""--> renaming the column names"""

#creating a list to rename the columns

list=['Cust_ID', 'INCOME', 'Loan_AMT', 'ANNUITY',
       'GOODS_PRICE', 'INCOME_TYPE', 'EDUCATION',
       'FAMILY_STATUS', 'curr_HOUSING_TYPE', 'age',
       'Work_Exp', 'REGISTRATION_change', 'DAYS_ID_PUBLISH', 'MOBIL_given',
       'EMAIL_given', 'OCCUPATION_TYPE', 'Family_MEMBERS_no',
       'REGION_CLIENT', 'REGION_CLIENT_CITY',
       'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',
       'ORGANIZATION_TYPE', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',
       'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'DAYS_LAST_PHONE_CHANGE']

# conactenating the list to new_df
new_df.columns=list

#checking if the column names have changed

new_df.columns

"""--> checking for columns with null values"""

#checking how many null values are present in each of the columns
#creating a function to find null values for the dataframe appl_data --> application_data
def null_values(appl_data):
    return round((appl_data.isnull().sum()*100/len(appl_data)).sort_values(ascending = False),2)

#displaying the column names and displaying the percentage of columns having null values
null_values(new_df)

#creating a function to find null values for the dataframe appl_data --> previous_data
def null_values(pa_df):
    return round((pa_df.isnull().sum()*100/len(pa_df)).sort_values(ascending = False),2)

#displaying the column names and displaying the percentage of columns having null values in newtest_df
null_values(newtest_df)

"""--> these are the columns having maximum null values


1.   RATE_INTEREST_PRIMARY       99.64
2. RATE_INTEREST_PRIVILEGED    99.64
3. AMT_DOWN_PAYMENT            53.64
4. RATE_DOWN_PAYMENT           53.64
5. INSURED_ON_APPROVAL         40.30
6. DAYS_TERMINATION            40.30
7. LAST_DUEDay                 40.30
8. FIRST_DUEDay                40.30
9. AMT_GOODS_PRICE             23.08
10. AMT_ANNUITY                 22.29


"""

#number of null values per column
print("missing values : ",newtest_df.isna().sum().sort_values(ascending = False))

newtest_df.describe(include='all')

"""#checking unique values for categorical columns and vizualizing data

"""

#importing vizualisation libraries
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

#unique value CONTRACT_TYPE
newtest_df.CONTRACT_TYPE.unique()

#count of unique value CONTRACT_TYPE  
ct=newtest_df.CONTRACT_TYPE.value_counts()
ct

#plotting bar chart for CONTRACT_TYPE categorical data
plt.barh(ct.index, ct)

# Plot clearly shows that, Customer taking cash loans and customer loans are more than compared to those taking revovling loans

#plotting pie chart for CONTRACT_TYPE categorical data
plt.pie(x=ct, labels=ct.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value CLIENT_TYPE
newtest_df.CLIENT_TYPE.unique()

#count of unique value CLIENT_TYPE  
clt= newtest_df.CLIENT_TYPE.value_counts()
clt

#plotting bar chart for CLIENT_TYPE categorical data
plt.barh(clt.index, clt)

# from the bar graph its concluded that those client who are repeaters are more as compared to new and refreshed clients

##plotting pie chart for CLIENT_TYPE categorical data
plt.pie(x=clt, labels=clt.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value GOODS_CATEGORY	
newtest_df.GOODS_CATEGORY.unique()

#count of unique value GOODS_CATEGORY  
gt=newtest_df.GOODS_CATEGORY.value_counts()
gt

#plotting bar chart for GOODS_CATEGORY categorical data
plt.barh(gt.index, gt)

# People taking loan for electronics equipment are more as compared to people taking loans for house construction or insurance.

#plotting pie chart for GOODS_CATEGORY categorical data
plt.pie(x=gt, labels=gt.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value PORTFOLIO	
newtest_df.PORTFOLIO.unique()

#count of unique value PORTFOLIO  
pf=newtest_df.PORTFOLIO.value_counts()
pf

#plotting bar chart for PORTFOLIO categorical data
plt.barh(pf.index, pf)

#plotting pie chart for PORTFOLIO categorical data
plt.pie(x=pf, labels=pf.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value CHANNEL_TYPE	
newtest_df.CHANNEL_TYPE.unique()

#count of unique value CHANNEL_TYPE  
cht=newtest_df.CHANNEL_TYPE.value_counts()
cht

#plotting bar chart for CHANNEL_TYPE categorical data
plt.barh(cht.index, cht)

#plotting pie chart for CHANNEL_TYPE categorical data
plt.pie(x=cht, labels=cht.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value SELLER_INDUSTRY	
newtest_df.SELLER_INDUSTRY.unique()

#count of unique value SELLER_INDUSTRY  
si=newtest_df.SELLER_INDUSTRY.value_counts()
si

#plotting bar chart for SELLER_INDUSTRY categorical data
plt.barh(si.index, si)

#plotting pie chart for SELLER_INDUSTRY categorical data
plt.pie(x=si, labels=si.index, autopct='%1.2f%%')
plt.legend()
plt.show()

#unique value YIELD_GROUP	
newtest_df.YIELD_GROUP.unique()

#count of unique value YIELD_GROUP  
yg=newtest_df.YIELD_GROUP.value_counts()
yg

#plotting bar chart for YIELD_GROUP categorical data
plt.barh(yg.index, yg)

#plotting pie chart for YIELD_GROUP categorical data
plt.pie(x=yg, labels=yg.index, autopct='%1.2f%%')
plt.legend()
plt.show()

"""--> for categorical data"""

#checking the data types of newtest data to find out the continuous categorical data to find the correlation
newtest_df.dtypes

#taking only continuous data into account
cat2_df=newtest_df[['CURR_CustID','AMT_ANNUITY', 'AMT_APPLICATION','AMT_CREDIT','AMT_GOODS_PRICE','FIRST_DUEDay','LAST_DUEDay','DAYS_TERMINATION','INSURED_ON_APPROVAL']]

cat2_df.head()

#Checking Corrilation Between columns of numeric continues data
cat2_df.corr()

#dropping the columns having null vvalues and creating new dataframe
cat2_df[['CURR_CustID','AMT_ANNUITY', 'AMT_APPLICATION','AMT_CREDIT','AMT_GOODS_PRICE']]

cat3_df=cat2_df[['CURR_CustID','AMT_ANNUITY', 'AMT_APPLICATION','AMT_CREDIT','AMT_GOODS_PRICE']].corr()
cat3_df

"""Conclusion--> 
1. AMT_ANNUITY is strongly correlated with
*   AMT_APPLICATION by corelation coefficient 0.808872	
*   AMT_CREDIT by corelation coefficient 0.816429
*   AMT_GOODS_PRICE by corelation coefficient 0.820895


2. AMT_APPLICATION is strongly correlated with	
*   AMT_CREDIT by corelation coefficient 0.975824
*   AMT_GOODS_PRICE by corelation coefficient 0.999884


3. AMT_CREDIT is strongly correlated with
*   AMT_GOODS_PRICE by corelation coefficient 0.993087

#ploting pairplot for continues data to check the retaion between columns
"""

#creating pairplot graph
sns.pairplot(data=cat3_df)

"""# Handling Outliers"""

# AMT_APPLICATION

#Checking outliers for INCOME
cat3_df.AMT_APPLICATION.describe()

sns.boxplot(cat3_df.AMT_APPLICATION)
plt.title('Distribution of AMT_APPLICATION')
plt.show()

# AMT_CREDIT

#Checking outliers for AMT_CREDIT
cat3_df.AMT_CREDIT.describe()

sns.boxplot(cat3_df.AMT_CREDIT)
plt.title('Distribution of AMT_CREDIT')
plt.show()

# AMT_GOODS_PRICE

#Checking outliers for AMT_GOODS_PRICE
cat3_df.AMT_GOODS_PRICE.describe()

plt.figure(figsize=(9,2))
sns.boxplot(cat3_df.AMT_GOODS_PRICE)
plt.title('Distribution of AMT_GOODS_PRICE')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

sns.regplot(x="AMT_ANNUITY", y="AMT_CREDIT", data=cat3_df)
plt.ylim(0,)
plt.show()

sns.regplot(x="AMT_ANNUITY", y="AMT_GOODS_PRICE", data=cat3_df)
plt.ylim(0,)
plt.show()

"""Q. Filterring data on bases of Loan amount for top 10 loan amounts"""

df_amt=cat2_df.nlargest(10,columns='AMT_APPLICATION')
df_amt.head()

#Plotting graph for approved loan amount 
df_amt.plot.bar()

"""'previous_application.csv' contains information about the clientâ€™s previous loan data. It contains the data whether the previous application had been Approved, Canceled, Refused or Unused offer.

"""

